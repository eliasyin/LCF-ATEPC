log file: logs/201023-142557.log
--------------------------------------------------------------------------------
Config 1 (totally 3 configs)
No.1 training process of 1
Model name '/home/ycf19/tools/pretrained_models/bert/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/ycf19/tools/pretrained_models/bert/bert-base-uncased' is a path or url to a directory containing tokenizer files.
Didn't find file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/added_tokens.json. We won't load it.
Didn't find file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/special_tokens_map.json. We won't load it.
Didn't find file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/tokenizer_config.json. We won't load it.
loading file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/vocab.txt
loading file None
loading file None
loading file None
loading configuration file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/config.json
Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

loading weights file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/pytorch_model.bin
>>> dataset: laptop
>>> output_dir: output
>>> SRD: 5
>>> learning_rate: 3e-05
>>> use_unique_bert: True
>>> use_bert_spc: False
>>> local_context_focus: fusion
>>> num_train_epochs: 5.0
>>> train_batch_size: 16
>>> dropout: 0.0
>>> max_seq_length: 80
>>> eval_batch_size: 32
>>> eval_steps: 20
>>> gradient_accumulation_steps: 1
>>> device: cuda:2
>>> seed: 1
>>> bert_model: /home/ycf19/tools/pretrained_models/bert/bert-base-uncased
>>> data_dir: atepc_datasets/laptop
***** Running training *****
  Num examples = 2327
  Batch size = 16
  Num steps = 725
################################################################################
Train 1 Epoch1
################################################################################
################################################################################
Train 1 Epoch2
################################################################################
################################################################################
Train 1 Epoch3
################################################################################
################################################################################
Train 1 Epoch4
################################################################################
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 75.16(max: 75.16)  APC_test_f1: 69.36(max: 69.36)
ATE_test_f1: 80.58(max:80.58)
********************************************************************************
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 76.1(max: 76.1)  APC_test_f1: 70.24(max: 70.24)
ATE_test_f1: 78.76(max:80.58)
********************************************************************************
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 77.83(max: 77.83)  APC_test_f1: 74.32(max: 74.32)
ATE_test_f1: 78.67(max:80.58)
********************************************************************************
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 74.21(max: 77.83)  APC_test_f1: 69.89(max: 74.32)
ATE_test_f1: 77.01(max:80.58)
********************************************************************************
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 75.79(max: 77.83)  APC_test_f1: 71.48(max: 74.32)
ATE_test_f1: 80.94(max:80.94)
********************************************************************************
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 76.26(max: 77.83)  APC_test_f1: 72.88(max: 74.32)
ATE_test_f1: 79.26(max:80.94)
********************************************************************************
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 76.42(max: 77.83)  APC_test_f1: 68.83(max: 74.32)
ATE_test_f1: 77.13(max:80.94)
********************************************************************************
********************************************************************************
Train 1 Epoch4, Evaluate for atepc_datasets/laptop
APC_test_acc: 73.27(max: 77.83)  APC_test_f1: 64.91(max: 74.32)
ATE_test_f1: 81.33(max:81.33)
********************************************************************************
################################################################################
Train 1 Epoch5
################################################################################
********************************************************************************
Train 1 Epoch5, Evaluate for atepc_datasets/laptop
APC_test_acc: 77.2(max: 77.83)  APC_test_f1: 72.5(max: 74.32)
ATE_test_f1: 81.09(max:81.33)
********************************************************************************
********************************************************************************
Train 1 Epoch5, Evaluate for atepc_datasets/laptop
APC_test_acc: 73.74(max: 77.83)  APC_test_f1: 71.23(max: 74.32)
ATE_test_f1: 80.47(max:81.33)
********************************************************************************
********************************************************************************
Train 1 Epoch5, Evaluate for atepc_datasets/laptop
APC_test_acc: 73.58(max: 77.83)  APC_test_f1: 67.2(max: 74.32)
ATE_test_f1: 75.79(max:81.33)
********************************************************************************
********************************************************************************
Train 1 Epoch5, Evaluate for atepc_datasets/laptop
APC_test_acc: 76.57(max: 77.83)  APC_test_f1: 73.09(max: 74.32)
ATE_test_f1: 80.06(max:81.33)
********************************************************************************
********************************************************************************
Train 1 Epoch5, Evaluate for atepc_datasets/laptop
APC_test_acc: 76.26(max: 77.83)  APC_test_f1: 73.22(max: 74.32)
ATE_test_f1: 81.41(max:81.41)
********************************************************************************
********************************************************************************
Train 1 Epoch5, Evaluate for atepc_datasets/laptop
APC_test_acc: 78.14(max: 78.14)  APC_test_f1: 74.28(max: 74.32)
ATE_test_f1: 80.62(max:81.41)
********************************************************************************
********************************************************************************
Train 1 Epoch5, Evaluate for atepc_datasets/laptop
APC_test_acc: 74.06(max: 78.14)  APC_test_f1: 72.46(max: 74.32)
ATE_test_f1: 80.45(max:81.41)
********************************************************************************
max_ate_test_f1:81.41 max_apc_test_acc: 78.14	max_apc_test_f1: 74.32 	
--------------------------------------------------------------------------------
Config 2 (totally 3 configs)
No.1 training process of 1
Model name '/home/ycf19/tools/pretrained_models/bert/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/ycf19/tools/pretrained_models/bert/bert-base-uncased' is a path or url to a directory containing tokenizer files.
Didn't find file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/added_tokens.json. We won't load it.
Didn't find file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/special_tokens_map.json. We won't load it.
Didn't find file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/tokenizer_config.json. We won't load it.
loading file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/vocab.txt
loading file None
loading file None
loading file None
loading configuration file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/config.json
Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

loading weights file /home/ycf19/tools/pretrained_models/bert/bert-base-uncased/pytorch_model.bin
>>> dataset: restaurant
>>> output_dir: output
>>> SRD: 5
>>> learning_rate: 3e-05
>>> use_unique_bert: True
>>> use_bert_spc: False
>>> local_context_focus: fusion
>>> num_train_epochs: 5.0
>>> train_batch_size: 16
>>> dropout: 0.0
>>> max_seq_length: 80
>>> eval_batch_size: 32
>>> eval_steps: 20
>>> gradient_accumulation_steps: 1
>>> device: cuda:2
>>> seed: 1
>>> bert_model: /home/ycf19/tools/pretrained_models/bert/bert-base-uncased
>>> data_dir: atepc_datasets/restaurant
***** Running training *****
  Num examples = 3602
  Batch size = 16
  Num steps = 1125
################################################################################
Train 1 Epoch1
################################################################################
################################################################################
Train 1 Epoch2
################################################################################
